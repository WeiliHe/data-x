{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.model_selection as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_df = pd.read_csv(\"lane_changing_features.csv\")\n",
    "labels_df = pd.read_csv(\"lane_changing_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(x, depth):\n",
    "    y = []\n",
    "    for i in range(len(x)):\n",
    "        a = np.zeros(depth)\n",
    "        a[x[i]] = 1\n",
    "        y.append(a)\n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = labels_df.values.tolist()\n",
    "features = features_df.values.tolist()\n",
    "labels_onehot = one_hot(labels, 3)\n",
    "X_train, X_test, y_train, y_test = sk.train_test_split(features,labels_onehot,test_size=0.30, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#inputs \n",
    "xs = tf.placeholder(tf.float32, [None, 20]) # 13 features in the sample\n",
    "ys = tf.placeholder(tf.float32, [None, 3]) # 3 outputs: keeping lane, left lane, right lane\n",
    "kp = tf.placeholder(tf.float32) # keep probability for dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Building neural network\n",
    "with tf.name_scope('layer1'): #visualize layer1\n",
    "    #Hidden layer 1\n",
    "    l1 = tf.layers.dense(xs, 40, activation = tf.nn.relu, name='layer1')\n",
    "    l1 = tf.layers.dropout(l1, name='dropout1', rate=kp) #dropout\n",
    "    layer1_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'layer1')\n",
    "    tf.summary.histogram('kernel', layer1_vars[0])\n",
    "    tf.summary.histogram('bias', layer1_vars[1])\n",
    "    tf.summary.histogram('act', l1)\n",
    "\n",
    "with tf.name_scope('layer2'): #visualize layer2\n",
    "    #Hidden layer 2\n",
    "    l2 = tf.layers.dense(l1, 60, activation=tf.nn.relu, name = 'layer2')\n",
    "    l2 = tf.layers.dropout(l2, name='dropout2', rate=kp) #dropout\n",
    "    layer2_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'layer2')\n",
    "    tf.summary.histogram('kernel', layer2_vars[0])\n",
    "    tf.summary.histogram('bias', layer2_vars[1])\n",
    "    tf.summary.histogram('act', l2)\n",
    "\n",
    "with tf.name_scope('layer3'): #visualize layer3\n",
    "    #Hidden layer 3\n",
    "    l3 = tf.layers.dense(l2, 40, activation=tf.nn.relu, name = 'layer3')\n",
    "    l3 = tf.layers.dropout(l3, name='dropout3', rate=kp) #dropout\n",
    "    layer3_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'layer3')\n",
    "    tf.summary.histogram('kernel', layer3_vars[0])\n",
    "    tf.summary.histogram('bias', layer3_vars[1])\n",
    "    tf.summary.histogram('act', l3)\n",
    "\n",
    "with tf.name_scope('output'): #visualize output layer\n",
    "    #Output layer\n",
    "    outputs = tf.layers.dense(l3, 3, activation=tf.nn.softmax, name='outputs')\n",
    "    outputs_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'outputs')\n",
    "    tf.summary.histogram('kernel', outputs_vars[0])\n",
    "    tf.summary.histogram('bias', outputs_vars[1])\n",
    "    tf.summary.histogram('act', outputs)\n",
    "    \n",
    "with tf.name_scope('loss'):  #visualize loss\n",
    "    #define loss function as cross entropy function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=outputs, labels=ys))\n",
    "    #define loss function as Mean Square Error\n",
    "    #loss = tf.reduce_mean(tf.squared_difference(ys, outputs))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \n",
    "with tf.name_scope('train'): #visualize optimizer\n",
    "    #use Adam Optimizer\n",
    "    train_step = tf.train.AdamOptimizer().minimize(loss)\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(outputs, 1), tf.argmax(ys, 1))\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define session\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for test set at step 0: 0.351213\n",
      "Accuracy for test set at step 10: 0.436902\n",
      "Accuracy for test set at step 20: 0.531493\n",
      "Accuracy for test set at step 30: 0.56065\n",
      "Accuracy for test set at step 40: 0.578678\n",
      "Accuracy for test set at step 50: 0.5927\n",
      "Accuracy for test set at step 60: 0.602048\n",
      "Accuracy for test set at step 70: 0.606499\n",
      "Accuracy for test set at step 80: 0.60917\n",
      "Accuracy for test set at step 90: 0.614289\n",
      "Accuracy for test set at step 100: 0.620743\n",
      "Accuracy for test set at step 110: 0.622969\n",
      "Accuracy for test set at step 120: 0.624304\n",
      "Accuracy for test set at step 130: 0.628533\n",
      "Accuracy for test set at step 140: 0.632094\n",
      "Accuracy for test set at step 150: 0.636546\n",
      "Accuracy for test set at step 160: 0.638104\n",
      "Accuracy for test set at step 170: 0.639217\n",
      "Accuracy for test set at step 180: 0.640997\n",
      "Accuracy for test set at step 190: 0.64122\n",
      "Accuracy for test set at step 200: 0.641665\n",
      "Accuracy for test set at step 210: 0.642332\n",
      "Accuracy for test set at step 220: 0.644558\n",
      "Accuracy for test set at step 230: 0.644781\n",
      "Accuracy for test set at step 240: 0.645226\n",
      "Accuracy for test set at step 250: 0.645894\n",
      "Accuracy for test set at step 260: 0.646116\n",
      "Accuracy for test set at step 270: 0.647006\n",
      "Accuracy for test set at step 280: 0.647452\n",
      "Accuracy for test set at step 290: 0.647674\n",
      "Accuracy for test set at step 300: 0.649455\n",
      "Accuracy for test set at step 310: 0.650345\n",
      "Accuracy for test set at step 320: 0.650345\n",
      "Accuracy for test set at step 330: 0.650345\n",
      "Accuracy for test set at step 340: 0.650345\n",
      "Accuracy for test set at step 350: 0.650345\n",
      "Accuracy for test set at step 360: 0.650568\n",
      "Accuracy for test set at step 370: 0.652571\n",
      "Accuracy for test set at step 380: 0.652571\n",
      "Accuracy for test set at step 390: 0.652571\n",
      "Accuracy for test set at step 400: 0.652571\n",
      "Accuracy for test set at step 410: 0.652793\n",
      "Accuracy for test set at step 420: 0.653238\n",
      "Accuracy for test set at step 430: 0.653683\n",
      "Accuracy for test set at step 440: 0.654129\n",
      "Accuracy for test set at step 450: 0.653906\n",
      "Accuracy for test set at step 460: 0.654351\n",
      "Accuracy for test set at step 470: 0.654574\n",
      "Accuracy for test set at step 480: 0.654574\n",
      "Accuracy for test set at step 490: 0.654796\n",
      "Accuracy for test set at step 500: 0.654796\n",
      "Accuracy for test set at step 510: 0.654796\n",
      "Accuracy for test set at step 520: 0.654796\n",
      "Accuracy for test set at step 530: 0.654796\n",
      "Accuracy for test set at step 540: 0.654796\n",
      "Accuracy for test set at step 550: 0.654796\n",
      "Accuracy for test set at step 560: 0.654796\n",
      "Accuracy for test set at step 570: 0.691298\n",
      "Accuracy for test set at step 580: 0.751391\n",
      "Accuracy for test set at step 590: 0.773203\n",
      "Accuracy for test set at step 600: 0.807701\n",
      "Accuracy for test set at step 610: 0.843757\n",
      "Accuracy for test set at step 620: 0.869352\n",
      "Accuracy for test set at step 630: 0.890051\n",
      "Accuracy for test set at step 640: 0.896283\n",
      "Accuracy for test set at step 650: 0.906299\n",
      "Accuracy for test set at step 660: 0.915869\n",
      "Accuracy for test set at step 670: 0.91765\n",
      "Accuracy for test set at step 680: 0.921878\n",
      "Accuracy for test set at step 690: 0.925217\n",
      "Accuracy for test set at step 700: 0.929001\n",
      "Accuracy for test set at step 710: 0.933229\n",
      "Accuracy for test set at step 720: 0.935233\n",
      "Accuracy for test set at step 730: 0.937236\n",
      "Accuracy for test set at step 740: 0.94191\n",
      "Accuracy for test set at step 750: 0.943468\n",
      "Accuracy for test set at step 760: 0.946361\n",
      "Accuracy for test set at step 770: 0.9497\n",
      "Accuracy for test set at step 780: 0.952148\n",
      "Accuracy for test set at step 790: 0.953483\n",
      "Accuracy for test set at step 800: 0.955041\n",
      "Accuracy for test set at step 810: 0.955931\n",
      "Accuracy for test set at step 820: 0.956599\n",
      "Accuracy for test set at step 830: 0.958157\n",
      "Accuracy for test set at step 840: 0.958825\n",
      "Accuracy for test set at step 850: 0.959047\n",
      "Accuracy for test set at step 860: 0.959715\n",
      "Accuracy for test set at step 870: 0.95927\n",
      "Accuracy for test set at step 880: 0.96016\n",
      "Accuracy for test set at step 890: 0.960605\n",
      "Accuracy for test set at step 900: 0.960828\n",
      "Accuracy for test set at step 910: 0.961051\n",
      "Accuracy for test set at step 920: 0.961051\n",
      "Accuracy for test set at step 930: 0.961273\n",
      "Accuracy for test set at step 940: 0.961718\n",
      "Accuracy for test set at step 950: 0.962163\n",
      "Accuracy for test set at step 960: 0.962163\n",
      "Accuracy for test set at step 970: 0.962386\n",
      "Accuracy for test set at step 980: 0.962831\n",
      "Accuracy for test set at step 990: 0.963276\n"
     ]
    }
   ],
   "source": [
    "#merge all te summaries and write them out to the folder\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter('logs/train', sess.graph)\n",
    "test_writer = tf.summary.FileWriter('logs/test')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(1000):\n",
    "    sess.run(train_step, feed_dict={xs: X_train, ys: y_train, kp: 0.5})\n",
    "    if i % 10 == 0:\n",
    "        train_result, acc1 = sess.run([merged, accuracy], feed_dict={xs: X_train, ys: y_train, kp: 1})\n",
    "        test_result, acc = sess.run([merged, accuracy], feed_dict={xs: X_test, ys: y_test, kp: 1})\n",
    "        train_writer.add_summary(train_result,i)\n",
    "        test_writer.add_summary(test_result, i)\n",
    "        #print('Accuracy for training set at step %s: %s' % (i, acc1))\n",
    "        print('Accuracy for test set at step %s: %s' % (i, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
